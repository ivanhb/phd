{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import requests\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Usefull functions\n",
    "\n",
    "def write_list(l,file_path, header= True):\n",
    "    f = open(file_path,\"w+\")\n",
    "    initial_pos = 0\n",
    "    \n",
    "    #header\n",
    "    if header:\n",
    "        initial_pos = 1\n",
    "        str_header = ''\n",
    "        for k_header in l[0].keys():\n",
    "            str_header = str_header + str(k_header) + \",\"\n",
    "        f.write(str_header[:-1]+\"\\n\")\n",
    "        \n",
    "    #content\n",
    "    for l_index in range(initial_pos,len(l)):\n",
    "        str_row = ''\n",
    "        for k_att in l[l_index]:\n",
    "            str_row = str_row + '\"'+str(l[l_index][k_att]) +'\"'+','\n",
    "        f.write(str_row[:-1]+\"\\n\")\n",
    "        \n",
    "\n",
    "def coci_call(operation, list_dois, fields):\n",
    "    items_dict = {}\n",
    "    for doi in list_dois:\n",
    "        r = requests.get('https://opencitations.net/index/coci/api/v1/'+str(operation)+\"/\"+str(doi))\n",
    "        if len(r.json()) > 0: \n",
    "            if fields == \"*\":\n",
    "                items_dict[doi] = r.json()[0]\n",
    "            else:\n",
    "                items_dict[doi] = {}\n",
    "                for f in fields:\n",
    "                    items_dict[doi][f] = None\n",
    "                    if f in r.json()[0]:\n",
    "                        items_dict[doi][f] = r.json()[0][f]\n",
    "    return items_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization\n",
    "\n",
    "def norm_isbn_code(x):\n",
    "    regex = r\"^([A-Z]{1,})\\d\"\n",
    "    matches = re.finditer(regex, x, re.MULTILINE)\n",
    "    for matchNum, match in enumerate(matches, start=1):\n",
    "        if match:\n",
    "            return match.groups()[0]\n",
    "        \n",
    "def norm_pdftext(t):\n",
    "    t = re.sub(r\"(\\w{1})\\-\\s(\\w{1})\", r\"\\1\\2\", t)\n",
    "    return t\n",
    "\n",
    "def norm_data(x):\n",
    "    x = x.rstrip().lstrip()\n",
    "    regex = r\"(\\d{4})\"\n",
    "    matches = re.finditer(regex, x, re.MULTILINE)\n",
    "    for matchNum, match in enumerate(matches, start=1):\n",
    "        if match:\n",
    "            return match.group()\n",
    "    return \"none\"\n",
    "\n",
    "def norm_source(x):\n",
    "    x = x.rstrip().lstrip().lower()\n",
    "    if x == \"doi.org\":\n",
    "        return \"doi\"\n",
    "    if x == \"other\":\n",
    "        return \"other\"\n",
    "    return \"none\"\n",
    "    \n",
    "def norm_title(x):\n",
    "    x = x.rstrip().lstrip().lower()\n",
    "    x = norm_pdftext(x)\n",
    "    return x\n",
    "\n",
    "def norm_abstract(x):\n",
    "    x = x.rstrip().lstrip().lower()\n",
    "    x = norm_pdftext(x)\n",
    "    return x\n",
    "\n",
    "def norm_section(x, intext_cits = None):\n",
    "    x = x.rstrip().lstrip()\n",
    "    sections = list(filter(None,[item for item in x.split(\";;\")])) \n",
    "    sections = [item.split(\";\") for item in sections]\n",
    "    for i,item_val in enumerate(sections): \n",
    "        for p,part_val in enumerate(item_val): \n",
    "            sections[i][p] = part_val.rstrip().lstrip().lower()\n",
    "            if sections[i][p] == \"none\":\n",
    "                return [[\"none\"] for j in range(0,intext_cits)]\n",
    "    return sections\n",
    "\n",
    "def norm_cits_text(x):\n",
    "    x = x.rstrip().lstrip()\n",
    "    cits_text = [norm_pdftext(item.rstrip().lstrip().lower()) for item in x.split(\";;\")]\n",
    "    cits_text = list(filter(None, cits_text))\n",
    "    return cits_text\n",
    "\n",
    "def norm_cit_intent(x):\n",
    "    x = x.rstrip().lstrip()\n",
    "    cit_intent = [item.rstrip().lstrip().lower() for item in x.split(\";;\")]\n",
    "    cit_intent = list(filter(None, cit_intent))\n",
    "    return cit_intent\n",
    "\n",
    "def norm_sentiment(x):\n",
    "    x = x.rstrip().lstrip()\n",
    "    sentiment = [item.rstrip().lstrip().lower() for item in x.split(\";;\")]\n",
    "    sentiment = list(filter(None, sentiment))\n",
    "    return sentiment\n",
    "\n",
    "def norm_retraction_men(x):\n",
    "    x = x.rstrip().lstrip().lower()\n",
    "    x = x.replace(\";;\",\"\")\n",
    "    return x\n",
    "\n",
    "def norm_note(x):\n",
    "    x = x.rstrip().lstrip()\n",
    "    note = [item.rstrip().lstrip().lower() for item in x.split(\";;\")]\n",
    "    note = list(filter(None, note))\n",
    "    return note\n",
    "\n",
    "## Normalize sources\n",
    "\n",
    "def norm_subject(x):\n",
    "    x = x.rstrip().lstrip()\n",
    "    norm_val = [item.rstrip().lstrip().lower() for item in x.split(\";;\")]\n",
    "    norm_val = list(filter(None, norm_val))\n",
    "    return norm_val\n",
    "\n",
    "def norm_area(x, intext_cits = None):\n",
    "    x = x.rstrip().lstrip()\n",
    "    norm_val = list(filter(None,[item for item in x.split(\";;\")])) \n",
    "    norm_val = [item.split(\";\") for item in norm_val]   \n",
    "    for i,item_val in enumerate(norm_val): \n",
    "        for p,part_val in enumerate(item_val): \n",
    "            norm_val[i][p] = part_val.rstrip().lstrip().lower()\n",
    "    return norm_val\n",
    "\n",
    "def norm_source_id(x):\n",
    "    def filter_null(x):\n",
    "        return x[0] != \"\"\n",
    "    \n",
    "    x = x.rstrip().lstrip()\n",
    "    norm_val = [item.rstrip().lstrip().lower() for item in x.split(\";\")]\n",
    "    norm_val = [tuple(item.split(\":\")) for item in norm_val]\n",
    "    norm_val = list(filter(filter_null, norm_val))\n",
    "    return norm_val\n",
    "\n",
    "def norm_dois(x):\n",
    "    x = x.rstrip().lstrip()\n",
    "    norm_val = [item.rstrip().lstrip().lower() for item in x.split(\"[[;;]]\")]\n",
    "    norm_val = list(filter(None, norm_val))\n",
    "    return norm_val\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Convert the Wakefield retraction in-text references dataset (\"coci_intext_ref.csv\") into the DASPLAB algorith input dataset format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| DASPLAB algorithm input dataset format| Meaning | Corresponding field from the in-text ref dataset |\n",
    "|--------------|-------------------------------------------------------------|---|\n",
    "| citfunc      | citation function                                           | cito_fun  |\n",
    "| sectitle     | the ecxact title of the section                             | section (part of it)  |\n",
    "| refentry     | the reference entry used in the citing article              | NONE  |\n",
    "| art          | the article identifier (e.g. DOI)                           | doi  |\n",
    "| sectype      | the section type (e.g. results, related work, introduction) | section (part of it)  |\n",
    "| sectype2     | _                                                           | NONE  |\n",
    "| refid        | reference URL                                               | NONE  |\n",
    "| sec          | section URL                                                 | NONE  |\n",
    "| ctx          | in-text ref URL                                             | NONE  |\n",
    "| pointerlist  | _                                                           | NONE  |\n",
    "| nbcontexts   | _                                                           | NONE  |\n",
    "| nbsections   | _                                                           | NONE  |\n",
    "| itrp         | URL of SOMETHING                                            | NONE  |\n",
    "| anchorsent   | the in-text reference anchor sentence                       | intext_ref  |\n",
    "| partext      | _                                                           | NONE  |\n",
    "| potential_cf | a possible Object-property of Cito                          | cito_fun  |\n",
    "| annotator    | _                                                           | NONE  |\n",
    "| dataset      | dataset name                                                | NONE  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data structures\n",
    "#---\n",
    "valid_docs = dict()\n",
    "\n",
    "with open(\"../wakefield_retraction/data/coci_intext_ref.csv\") as a_file:\n",
    "    csv_reader = csv.reader(a_file, delimiter=',')\n",
    "    # skip the headers \n",
    "    # 0.Date,\n",
    "    # 1.DOI\n",
    "    # 2.Source\n",
    "    # 3.Title\n",
    "    # 4.Abstract\n",
    "    # 5.Section\n",
    "    # 6.Citations to retracted article\n",
    "    # 7.Citing reasons,\n",
    "    # 8.Sentiment (negative/neutral/positive)\n",
    "    # 9.Mentions the article retraction,\n",
    "    # 10.Is a retracted article\n",
    "    # 11.Notes\n",
    "    \n",
    "    #skip the header\n",
    "    next(csv_reader, None)\n",
    "    \n",
    "    #iterate all the csv rows\n",
    "    for row in csv_reader:\n",
    "        \n",
    "        #Classify documents into Vlaid and Error \n",
    "        #---\n",
    "        cits_text = norm_cits_text(row[6])\n",
    "        if len(cits_text) == 0:\n",
    "            err_docs[row[1]] = row[10]\n",
    "        else:\n",
    "            doi = row[1]\n",
    "            valid_docs[doi] = dict()\n",
    "            valid_docs[doi][\"year\"] = norm_data(row[0])\n",
    "            valid_docs[doi][\"source\"] = norm_source(row[2])\n",
    "            valid_docs[doi][\"title\"] = norm_title(row[3])\n",
    "            valid_docs[doi][\"abstract\"] = norm_abstract(row[4])\n",
    "            valid_docs[doi][\"cits_text\"] = cits_text\n",
    "            valid_docs[doi][\"section\"] = norm_section(row[5], len(cits_text))\n",
    "            valid_docs[doi][\"cit_intent\"] = norm_cit_intent(row[7])\n",
    "            valid_docs[doi][\"sentiment\"] = norm_sentiment(row[8])\n",
    "            valid_docs[doi][\"retraction_mention\"] = norm_retraction_men(row[9])\n",
    "            valid_docs[doi][\"note\"] = norm_note(row[10])\n",
    "            valid_docs[doi][\"retracted\"] = norm_retraction_men(row[10])\n",
    "            valid_docs[doi][\"note\"] = norm_note(row[11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(valid_docs).transpose()\n",
    "df[\"intext_cit\"] = list(zip(df[\"cits_text\"],df[\"cit_intent\"],df[\"sentiment\"],df[\"section\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "dasplab_alg_input = []\n",
    "for doi, item in df.iterrows():\n",
    "    \n",
    "    for i in range(0,len(item[\"intext_cit\"][0])):\n",
    "                   \n",
    "        #define section parts \n",
    "        sec_parts = {\"sectitle\":\"\",\"sectype\":\"\"}\n",
    "        for s_ann in item[\"intext_cit\"][3]: \n",
    "            for s_p in s_ann:\n",
    "                if \"introduction\" in s_p:\n",
    "                    sec_parts[\"sectype\"] = \"introduction\"\n",
    "                elif \"background\" in s_p:\n",
    "                    sec_parts[\"sectype\"] = \"related work\"\n",
    "                elif \"method\" in s_p:\n",
    "                    sec_parts[\"sectype\"] = \"methods\"\n",
    "                elif \"result\" in s_p:\n",
    "                    sec_parts[\"sectype\"] = \"results\"\n",
    "                elif \"conclusion\" in s_p:\n",
    "                    sec_parts[\"sectype\"] = \"conclusions\"\n",
    "                if s_p != \"\" and s_p[0] == 'â€œ':\n",
    "                    sec_parts[\"sectitle\"] = s_p[1:-1]\n",
    "        \n",
    "        rep_item = {\n",
    "            \"citfunc\" : item[\"intext_cit\"][1][i] ,\n",
    "            \"sectitle\" :  sec_parts[\"sectitle\"],\n",
    "            \"refentry\" :  \"\",\n",
    "            \"art\" :  doi,\n",
    "            \"sectype\" :  sec_parts[\"sectype\"],\n",
    "            \"sectype2\" :\"\",\n",
    "            \"refid\" :  \"\",\n",
    "            \"sec\" :  \"\",\n",
    "            \"ctx\" :  \"\",\n",
    "            \"pointerlist\" :  \"\",\n",
    "            \"nbcontexts\" :  \"\",\n",
    "            \"nbsections\" :  \"\",\n",
    "            \"itrp\" :  \"\",\n",
    "            \"anchorsent\" : item[\"intext_cit\"][0][i],\n",
    "            \"partext\" :  \"\",\n",
    "            \"potential_cf\" : \"\",\n",
    "            \"annotator\" : item[\"intext_cit\"][1][i] ,\n",
    "            \"dataset\" : \"coci_wakefield_ret_intext_ref.csv\"\n",
    "        }\n",
    "        dasplab_alg_input.append(rep_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_format = pd.DataFrame.from_dict(dasplab_alg_input)\n",
    "write_list(dasplab_alg_input, \"wakefield_intext_ref.csv\", header= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
